%% LyX 2.3.6.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{array}
\usepackage{fancybox}
\usepackage{calc}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\onehalfspacing

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{xca}[thm]{\protect\exercisename}
\theoremstyle{remark}
\newtheorem*{rem*}{\protect\remarkname}

\makeatother

\usepackage{babel}
\providecommand{\exercisename}{Exercise}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\title{Compressed Sensing -- Sheet 12}
\author{Ferdinand Vanmaele}
\maketitle
\begin{xca}[Programming project part 1]
\mbox{}
\begin{itemize}
\item The generation of sensors is done in \texttt{sheet12\_ex1/sensors.py}.
To make the matrices work with the assumptions of CoSaMP, OMP, SP
and MP as defined in the lecture, the columns are normalized.
\begin{itemize}
\item $m$ rows are drawn uniformly at random (without repetition) by generating
a random permutation $S(m)\rightarrow S(m)$.
\end{itemize}
\item Random $s$-sparse vectors $x\in\mathbb{R}^{n}$ with $1\leq s\leq m$
are constructed in \texttt{sheet12\_ex1/main.py}. The \texttt{generate\_problems()}
function takes the problem dimension $n$ and bounds for $s$, as
well as the number repetitions for each sparsity value.
\item Algorithms are run in \texttt{sheet12\_ex1/main.py}. Every algorithm
is implemented in its own function\footnote{This is a bit repetitive, especially for closely related algorithms
such as CoSaMP and SP. However, every different algorithm to be tested
is self-contained in this way. Outside of this exercise, I would likely
summarize some functions and add additional parameters.} and can be enabled or disabled selectively.

\noindent\shadowbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule - \shadowsize}%
\begin{rem*}
The following modifications were made:
\begin{itemize}
\item A tolerance of $10^{-8}$ was chosen te reduce oscillations for different
sparsity levels.
\item Iterative hard thresholding was implemented with a variable step size
{[}TBDR12{]}, 
\[
g:=A^{\ast}r^{(k)},\quad T:=\begin{cases}
\text{supp}(\mathcal{T}(g,s)) & \text{if }k>0\\
\text{supp}(\mathcal{T}(x^{(0)},s)) & \text{if }k=0
\end{cases},\quad\mu:=\frac{\|g_{T}\|_{2}^{2}}{\|A_{T}g_{T}\|_{2}^{2}}
\]
instead of $\mu=1$. In the experiments performed, this was done to
avoid divergence with $\mu=1$.
\end{itemize}
\end{rem*}
%
\end{minipage}}
\item Let $\text{RE}(\hat{x}):=\|x-\hat{x}\|_{2}/\|x\|_{2}$. We consider
successful recovery of a vector of sparsity $1\leq s\leq m$ as the
maximum $s$, such that
\[
s_{\max}:=\max_{s}\left\{ \forall s'\leq s:\,\frac{1}{100}\sum_{i\in[100]}\text{RE}(\hat{x}_{s',i})<10^{-6}\right\} 
\]
holds. Summary of the trials, with $n=128$, $m=2^{7}$:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
Matrix &  & Algorithm & $s_{\max}$ & $\approx\sum\text{RE}$\tabularnewline
\hline 
\hline 
\multirow{8}{*}{\textbf{Random }($A$)} & 1 & $\ell_{1}$-min. (BP) & $s=17$ & $10^{-9}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 2 & OMP & $s=13$ & $10^{-15}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 3 & MP & $s=5$ & $10^{-9}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 4 & IHT & $s=10$ & $10^{-9}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 5 & CoSaMP & $s=14$ & $10^{-15}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 6 & BT & $s=0$ & $-$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 7 & HTP & $s=10$ & $10^{-15}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 8 & SP & $s=15$ & $10^{-15}$\tabularnewline
\hline 
\multirow{8}{*}{\textbf{Fourier }($F$)} & 1 & $\ell_{1}$-min. (BP) & $s=40$ & $10^{-10}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 2 & OMP & $s=34$ & $10^{-15}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 3 & MP & $s=10$ & $10^{-7}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 4 & IHT & $s=26$ & $10^{-9}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 5 & CoSaMP & $s=25$ & $10^{-15}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 6 & BT & $s=0$ & $-$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 7 & HTP & $s=26$ & $10^{-15}$\tabularnewline
\cline{2-5} \cline{3-5} \cline{4-5} \cline{5-5} 
 & 8 & SP & $s=31$ & $10^{-15}$\tabularnewline
\hline 
\end{tabular}
\par\end{center}
\item Conclusions:
\begin{itemize}
\item Methods using orthogonal projection recover the solution at a high
accuracy (close to machine precision) for $1\leq s$ sufficiently
small. IHT is still competitive in terms of $s_{\max}$, with a higher
recovery level.
\item The recovery error increased steeply from $s_{\max}$ to higher levels
$s>s_{\max}$. Some trials also had oscillatory behavior.
\item The Fourier matrix was recovered for higher sparsity levels for all
algorithms except Basic Thresholding. The coherence of this matrix
is much lower than the Gaussian one ($\approx0.08$ vs. $\approx1.00$)
so this is unsurprising.
\item $\ell_{1}$-minimization had constant performance for all trials.
Methods using orthogonal projection had their CPU time increase exponentially.
(See Figure \ref{fig:Combined-CPU-time-Gaussian} and \ref{fig:Combined-CPU-time-Fourier})
\end{itemize}
\end{itemize}
\end{xca}

\begin{figure}
\begin{centering}
\includegraphics[width=0.9\columnwidth]{A_m64_n128_100_error}
\par\end{centering}
\caption{Relative recovery error ($y$-axis) over 100 trials per sparsity $1\protect\leq s\protect\leq m$
($x$-axis) for \textbf{Gaussian} \textbf{matrix} $A\in\mathbb{R}^{64\times128}$
with normalized columns.\label{fig:Relative-recovery-error-Gaussian}}
\end{figure}

\begin{figure}
\begin{centering}
\includegraphics[width=0.8\columnwidth]{F_m64_n128_100_error}
\par\end{centering}
\caption{Relative recovery error ($y$-axis) averaged over 100 trials, by sparsity
level $1\protect\leq s\protect\leq m$ ($x$-axis) for \textbf{partial
Fourier} \textbf{matrix} $F\in\mathbb{R}^{64\times128}$ with normalized
columns.\label{fig:Relative-recovery-error-Fourier}}
\end{figure}

\begin{figure}
\begin{centering}
\includegraphics[width=0.8\columnwidth]{A_m64_n128_100_cputime}
\par\end{centering}
\caption{Combined CPU time in seconds per iteration ($y$-axis) averaged over
100 trials, by sparsity level $1\protect\leq s\protect\leq m$ for
\textbf{Gaussian matrix} $A\in\mathbb{R}^{64\times128}$ with normalized
columns.\label{fig:Combined-CPU-time-Gaussian}}

\end{figure}

\begin{figure}
\begin{centering}
\includegraphics[width=0.8\columnwidth]{F_m64_n128_100_cputime}
\par\end{centering}
\caption{Combined CPU time in seconds per iteration ($y$-axis) averaged over
100 trials, by sparsity level $1\protect\leq s\protect\leq m$ for
\textbf{partial Fourier matrix} $F\in\mathbb{R}^{64\times128}$ with
normalized columns.\label{fig:Combined-CPU-time-Fourier}}

\end{figure}

\newpage{}
\begin{xca}
The robust face recognition problem is given by
\[
\min_{w\in\mathbb{R}^{m+n}}\frac{1}{2}\|Bw-b\|_{2}^{2}+\lambda\|w\|_{1}
\]
where
\begin{itemize}
\item $\lambda\in\mathbb{R}_{>0}$ is a regularization parameter, 
\item $B=[A\quad I]\in\mathbb{R}^{m\times(m+n)}$ with $A\in\mathbb{R}^{m\times n}$,
$I\in\mathbb{R}^{m\times m}$ a highly correlated dictionary of $m$
face images $v\in\mathbb{R}^{n}$, stacked column-wise;
\item $I$ the identity matrix $\in\mathbb{R}^{n\times n}$, $b=Ax+e$ a
face image to be recognized not in the dictionary $A$, with $e\in\mathbb{R}^{n}$
an unknown error;
\item $w$ the minimization variable. 
\end{itemize}
The term $F(w):=\frac{1}{2}\|Bw-b\|_{2}^{2}$ is differentiable,
\begin{equation}
\nabla F(w)=B^{T}(Bw-b)\label{eq:minimization}
\end{equation}
with Lipschitz constant $L:=\|B^{T}B\|_{F}$,
\begin{align*}
\|B^{T}(Bw-b)-B^{T}(Bz-b)\|_{2} & =\|B^{T}Bw-B^{T}b-B^{T}Bz+B^{T}b\|_{2}\\
 & =\|B^{T}Bw-B^{T}Bz\|_{2}\\
 & \leq\|B^{T}B\|_{F}\|w-z\|_{2}.
\end{align*}
The norm $\|w\|_{1}$ is proper, convex and lsc and has as proximal
operator \textbf{soft shrinkage}, given component-wise by
\[
\left[\text{Prox}_{\gamma\|\cdot\|}(w)\right]_{i}=\begin{cases}
w_{i}+\gamma & \text{if }w_{i}<\gamma,\\
0 & \text{if }-\gamma\leq w_{i}\leq\gamma,\\
w_{i}-\gamma & \text{if }w_{i}>\gamma.
\end{cases}
\]
Multiplying by $\lambda>0$ we have:
\begin{align*}
\left[\text{Prox}_{\gamma R}(w)\right]_{i} & =\lambda^{-1}\left[\text{Prox}_{\lambda^{2}\gamma\|\cdot\|}(\lambda w)\right]_{i}\\
 & =\begin{cases}
w_{i}+\lambda\gamma & \text{if }w_{i}<\lambda\gamma,\\
0 & \text{if }-\lambda\gamma\leq w_{i}\leq\lambda\gamma,\\
w_{i}-\lambda\gamma & \text{if }w_{i}>\lambda\gamma.
\end{cases}
\end{align*}

The non-zero entries of $x_{\ast}$ in the solution $w_{\ast}=[x_{\ast}^{T},e_{\ast}^{T}]^{T}$
of problem (\ref{eq:minimization}) represent the person the image
belongs to.
\begin{rem*}
When we consider images without noise, we have the optimization problem
\[
\min_{x\in\mathbb{R}^{m}}\frac{1}{2}\|Ax-b\|_{2}^{2}+\lambda\|x\|_{1}
\]
with gradient $\nabla F(w)=A^{T}(Ax-b)$ and proximal operator as
above. The non-zero entries of the solution $x_{\ast}$ represent
the person the image belongs to.
\end{rem*}
\end{xca}


\end{document}
